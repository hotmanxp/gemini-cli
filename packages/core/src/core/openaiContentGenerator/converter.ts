/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import type {
  GenerateContentParameters,
  Part,
  Content,
  Tool,
  ToolListUnion,
  CallableTool,
  FunctionResponse,
  ContentUnion,
  Candidate,
} from '@google/genai';
import { GenerateContentResponse, FinishReason } from '@google/genai';
import type OpenAI from 'openai';
import { safeJsonParse } from '../../utils/safeJsonParse.js';
import { StreamingToolCallParser } from './streamingToolCallParser.js';

/**
 * Extended usage type that supports both OpenAI standard format and alternative formats
 * Some models return cached_tokens at the top level instead of in prompt_tokens_details
 */
interface ExtendedCompletionUsage extends OpenAI.CompletionUsage {
  cached_tokens?: number;
}

interface ExtendedChatCompletionAssistantMessageParam
  extends OpenAI.Chat.ChatCompletionAssistantMessageParam {
  reasoning_content?: string | null;
}

export interface ExtendedCompletionMessage
  extends OpenAI.Chat.ChatCompletionMessage {
  reasoning_content?: string | null;
  reasoning?: string | null;
}

export interface ExtendedCompletionChunkDelta
  extends OpenAI.Chat.ChatCompletionChunk.Choice.Delta {
  reasoning_content?: string | null;
  reasoning?: string | null;
}

/**
 * Tool call accumulator for streaming responses
 */
export interface ToolCallAccumulator {
  id?: string;
  name?: string;
  arguments: string;
}

type OpenAIContentPartVideoUrl = {
  type: 'video_url';
  video_url: {
    url: string;
  };
};

type OpenAIContentPartFile = {
  type: 'file';
  file: {
    filename: string;
    file_data: string;
  };
};

type OpenAIContentPart =
  | OpenAI.Chat.ChatCompletionContentPartText
  | OpenAI.Chat.ChatCompletionContentPartImage
  | OpenAI.Chat.ChatCompletionContentPartInputAudio
  | OpenAIContentPartVideoUrl
  | OpenAIContentPartFile;

/**
 * Converter class for transforming data between Gemini and OpenAI formats
 */
export class OpenAIContentConverter {
  private model: string;
  private streamingToolCallParser: StreamingToolCallParser =
    new StreamingToolCallParser();
  // Store tool parameter schemas for parameter name mapping
  private toolParameterSchemas: Map<string, Record<string, unknown>> =
    new Map();

  constructor(model: string) {
    this.model = model;
  }

  /**
   * Update the model used for response metadata (modelVersion/logging) and any
   * model-specific conversion behavior.
   */
  setModel(model: string): void {
    this.model = model;
  }

  /**
   * Store tool parameter schemas for parameter name mapping during tool call conversion.
   * This is called when tools are converted from Gemini to OpenAI format.
   */
  setToolParameterSchemas(tools: OpenAI.Chat.ChatCompletionTool[]): void {
    this.toolParameterSchemas.clear();
    for (const tool of tools) {
      if (tool.function?.name && tool.function?.parameters) {
        this.toolParameterSchemas.set(
          tool.function.name,
          tool.function.parameters as Record<string, unknown>,
        );
      }
    }
  }

  /**
   * Map tool call arguments to match the expected parameter names.
   * This handles cases where model outputs different parameter names than expected.
   * For example, maps 'path' to 'dir_path' for the list_directory tool.
   */
  private mapToolCallArgs(
    toolName: string,
    args: Record<string, unknown>,
  ): Record<string, unknown> {
    const schema = this.toolParameterSchemas.get(toolName);
    if (!schema) {
      return args;
    }

    const requiredParams = (schema['required'] as string[]) || [];
    const properties = (schema['properties'] as Record<string, unknown>) || {};
    const propertyNames = new Set(Object.keys(properties));

    // Create a mapping from common alternative names to expected names
    // This handles cases where models use different parameter names
    const paramNameMapping: Record<string, string> = {
      // list_directory tool: path -> dir_path
      path: 'dir_path',
      // Common mappings for other tools
      file_path: 'file_path',
      query: 'query',
      pattern: 'pattern',
      dir_path: 'dir_path',
      content: 'content',
      text: 'text',
    };

    const mappedArgs: Record<string, unknown> = {};

    // First, copy all args that are already valid parameter names
    for (const key of Object.keys(args)) {
      if (propertyNames.has(key)) {
        mappedArgs[key] = args[key];
      }
    }

    // Then, map alternative names to expected names for required parameters
    for (const requiredParam of requiredParams) {
      if (mappedArgs[requiredParam] === undefined) {
        // Find if there's an alternative name for this parameter
        for (const [altName, expectedName] of Object.entries(
          paramNameMapping,
        )) {
          if (
            expectedName === requiredParam &&
            args[altName] !== undefined &&
            !propertyNames.has(altName)
          ) {
            mappedArgs[requiredParam] = args[altName];
            break;
          }
        }
      }
    }

    return mappedArgs;
  }

  /**
   * Reset streaming tool calls parser for new stream processing
   * This should be called at the beginning of each stream to prevent
   * data pollution from previous incomplete streams
   */
  resetStreamingToolCalls(): void {
    this.streamingToolCallParser.reset();
  }

  /**
   * Convert Gemini tool parameters to OpenAI JSON Schema format
   */
  convertGeminiToolParametersToOpenAI(
    parameters: Record<string, unknown>,
  ): Record<string, unknown> | undefined {
    if (!parameters || typeof parameters !== 'object') {
      return parameters;
    }

    const converted = JSON.parse(JSON.stringify(parameters));

    const convertTypes = (obj: unknown): unknown => {
      if (typeof obj !== 'object' || obj === null) {
        return obj;
      }

      if (Array.isArray(obj)) {
        return obj.map(convertTypes);
      }

      const result: Record<string, unknown> = {};
      for (const [key, value] of Object.entries(obj)) {
        if (key === 'type' && typeof value === 'string') {
          // Convert Gemini types to OpenAI JSON Schema types
          const lowerValue = value.toLowerCase();
          if (lowerValue === 'integer') {
            result[key] = 'integer';
          } else if (lowerValue === 'number') {
            result[key] = 'number';
          } else {
            result[key] = lowerValue;
          }
        } else if (
          key === 'minimum' ||
          key === 'maximum' ||
          key === 'multipleOf'
        ) {
          // Ensure numeric constraints are actual numbers, not strings
          if (typeof value === 'string' && !isNaN(Number(value))) {
            result[key] = Number(value);
          } else {
            result[key] = value;
          }
        } else if (
          key === 'minLength' ||
          key === 'maxLength' ||
          key === 'minItems' ||
          key === 'maxItems'
        ) {
          // Ensure length constraints are integers, not strings
          if (typeof value === 'string' && !isNaN(Number(value))) {
            result[key] = parseInt(value, 10);
          } else {
            result[key] = value;
          }
        } else if (typeof value === 'object') {
          result[key] = convertTypes(value);
        } else {
          result[key] = value;
        }
      }
      return result;
    };

    return convertTypes(converted) as Record<string, unknown> | undefined;
  }

  /**
   * Convert Gemini tools to OpenAI format for API compatibility.
   * Handles both Gemini tools (using 'parameters' field) and MCP tools (using 'parametersJsonSchema' field).
   */
  async convertGeminiToolsToOpenAI(
    geminiTools: ToolListUnion,
  ): Promise<OpenAI.Chat.ChatCompletionTool[]> {
    const openAITools: OpenAI.Chat.ChatCompletionTool[] = [];

    for (const tool of geminiTools) {
      let actualTool: Tool;

      if ('functionDeclarations' in tool) {
        actualTool = tool;
      } else if ('googleSearch' in tool) {
        // Skip Google Search tools for OpenAI compatibility
        continue;
      } else if ('codeExecution' in tool) {
        // Skip code execution tools for OpenAI compatibility
        continue;
      } else if ('callableTool' in tool) {
        const callableTool = tool as CallableTool;
        // Convert callable tools to function declarations
        // Note: The actual property name may vary depending on the SDK version
        const toolObj = callableTool as unknown as Record<string, unknown>;
        const toolId = toolObj?.['id'] as string | undefined;
        const toolDescription = toolObj?.['description'] as string | undefined;
        const toolParams = toolObj?.['parameters'] as Record<string, unknown> | undefined;

        actualTool = {
          functionDeclarations: [
            {
              name: toolId || 'unknown_tool',
              description: toolDescription || 'A callable tool',
              parameters: toolParams,
            },
          ],
        };
      } else {
        // Unknown tool type, skip
        continue;
      }

      if (actualTool.functionDeclarations) {
        for (const func of actualTool.functionDeclarations) {
          const openAITool: OpenAI.Chat.ChatCompletionTool = {
            type: 'function',
            function: {
              name: func.name || 'unknown_function',
              description: func.description,
              parameters: this.convertGeminiToolParametersToOpenAI(
                func.parameters as Record<string, unknown>,
              ) as OpenAI.FunctionParameters,
            },
          };
          openAITools.push(openAITool);
        }
      }
    }

    // Store tool parameter schemas for parameter name mapping during tool call conversion
    this.setToolParameterSchemas(openAITools);

    return openAITools;
  }

  /**
   * Convert Gemini request to OpenAI messages format
   */
  convertGeminiRequestToOpenAI(
    request: GenerateContentParameters,
  ): OpenAI.Chat.ChatCompletionMessageParam[] {
    const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [];

    // Handle system instruction if present
    if (request.config?.systemInstruction) {
      const systemInstruction = request.config.systemInstruction;
      let systemContent = '';

      if (typeof systemInstruction === 'string') {
        systemContent = systemInstruction;
      } else if ('parts' in systemInstruction && systemInstruction.parts) {
        systemContent = systemInstruction.parts
          .map((part) => (typeof part === 'string' ? part : part.text || ''))
          .join('');
      }

      if (systemContent) {
        messages.push({
          role: 'system',
          content: systemContent,
        });
      }
    }

    // Convert contents to OpenAI messages
    const contents = request.contents;

    if (Array.isArray(contents)) {
      for (const content of contents) {
        this.convertContentToMessages(content, messages);
      }
    } else if (contents) {
      this.convertContentToMessages(contents, messages);
    }

    // Clean up orphaned tool calls and merge consecutive assistant messages
    const cleaned = this.cleanOrphanedToolCalls(messages);
    const merged = this.mergeConsecutiveAssistantMessages(cleaned);

    return merged;
  }

  private convertContentToMessages(
    content: ContentUnion,
    messages: OpenAI.Chat.ChatCompletionMessageParam[],
  ): void {
    if (typeof content === 'string') {
      messages.push({
        role: 'user',
        content,
      });
      return;
    }

    if (!('role' in content) || !('parts' in content)) {
      return;
    }

    const geminiContent = content as Content;
    const role = geminiContent.role;
    const parts = geminiContent.parts || [];

    if (role === 'user') {
      const contentParts: OpenAIContentPart[] = [];

      for (const part of parts) {
        if (typeof part === 'string') {
          contentParts.push({ type: 'text', text: part });
        } else if ('text' in part && part.text) {
          contentParts.push({ type: 'text', text: part.text });
        } else {
          const mediaPart = this.createMediaContentPart(part);
          if (mediaPart) {
            contentParts.push(mediaPart);
          }
        }
      }

      if (contentParts.length > 0) {
        messages.push({
          role: 'user',
          content: contentParts as unknown as OpenAI.Chat.ChatCompletionContentPart[],
        });
      }
    } else if (role === 'model') {
      const textParts: string[] = [];
      const toolCalls: OpenAI.Chat.ChatCompletionMessageToolCall[] = [];
      const reasoningParts: string[] = [];

      for (const part of parts) {
        if (typeof part === 'string') {
          textParts.push(part);
        } else if ('text' in part && part.text) {
          if (part.thought) {
            reasoningParts.push(part.text);
          } else {
            textParts.push(part.text);
          }
        } else if ('functionCall' in part && part.functionCall) {
          toolCalls.push({
            id: part.functionCall.id || `call_${Date.now()}`,
            type: 'function',
            function: {
              name: part.functionCall.name || 'unknown_function',
              arguments: JSON.stringify(part.functionCall.args || {}),
            },
          });
        }
      }

      const assistantMessage: ExtendedChatCompletionAssistantMessageParam = {
        role: 'assistant',
        content: textParts.join('') || null,
      };

      if (toolCalls.length > 0) {
        assistantMessage.tool_calls = toolCalls;
      }

      const reasoningContent = reasoningParts.join('');
      if (reasoningContent) {
        assistantMessage.reasoning_content = reasoningContent;
      }

      messages.push(assistantMessage);
      return;
    }

    // Handle function responses (tool responses)
    if (Array.isArray(geminiContent.parts)) {
      for (const part of geminiContent.parts) {
        if (typeof part !== 'string' && 'functionResponse' in part) {
          const toolMessage = this.createToolMessage(
            part.functionResponse as FunctionResponse,
          );
          if (toolMessage) {
            messages.push(toolMessage);
          }
        }
      }
    }
  }

  private extractFunctionResponseContent(response: unknown): string {
    if (response === null || response === undefined) {
      return '';
    }

    if (typeof response === 'string') {
      return response;
    }

    if (typeof response === 'object') {
      const responseObject = response as Record<string, unknown>;
      const output = responseObject['output'];
      if (typeof output === 'string') {
        return output;
      }

      const error = responseObject['error'];
      if (typeof error === 'string') {
        return error;
      }
    }

    try {
      const serialized = JSON.stringify(response);
      return serialized ?? String(response);
    } catch {
      return String(response);
    }
  }

  /**
   * Create a tool message from function response (with embedded media parts)
   */
  private createToolMessage(
    response: FunctionResponse,
  ): OpenAI.Chat.ChatCompletionToolMessageParam | null {
    const textContent = this.extractFunctionResponseContent(response.response);
    const contentParts: OpenAIContentPart[] = [];

    // Add text content first if present
    if (textContent) {
      contentParts.push({ type: 'text' as const, text: textContent });
    }

    // Add media parts from function response
    for (const part of response.parts || []) {
      const mediaPart = this.createMediaContentPart(part);
      if (mediaPart) {
        contentParts.push(mediaPart);
      }
    }

    // IMPORTANT: Always return a tool message, even if content is empty
    // OpenAI API requires that every tool call has a corresponding tool response
    // Empty tool results are valid (e.g., reading an empty file, successful operations with no output)
    if (contentParts.length === 0) {
      // Return empty string for empty tool results
      return {
        role: 'tool' as const,
        tool_call_id: response.id || '',
        content: '',
      };
    }

    // Cast to OpenAI type - some OpenAI-compatible APIs support richer content in tool messages
    return {
      role: 'tool' as const,
      tool_call_id: response.id || '',
      content: contentParts as unknown as
        | string
        | OpenAI.Chat.ChatCompletionContentPartText[],
    };
  }

  /**
   * Create OpenAI media content part from Gemini part
   */
  private createMediaContentPart(part: Part): OpenAIContentPart | null {
    if (part.inlineData?.mimeType && part.inlineData?.data) {
      const mimeType = part.inlineData.mimeType;
      const mediaType = this.getMediaType(mimeType);
      if (mediaType === 'image') {
        const dataUrl = `data:${mimeType};base64,${part.inlineData.data}`;
        return {
          type: 'image_url' as const,
          image_url: { url: dataUrl },
        };
      }

      if (mimeType === 'application/pdf') {
        const filename = part.inlineData.displayName || 'document.pdf';
        return {
          type: 'file' as const,
          file: {
            filename,
            file_data: `data:${mimeType};base64,${part.inlineData.data}`,
          },
        };
      }

      if (mediaType === 'audio') {
        const format = this.getAudioFormat(mimeType);
        if (format) {
          return {
            type: 'input_audio' as const,
            input_audio: {
              data: `data:${mimeType};base64,${part.inlineData.data}`,
              format,
            },
          };
        }
      }

      if (mediaType === 'video') {
        return {
          type: 'video_url' as const,
          video_url: {
            url: `data:${mimeType};base64,${part.inlineData.data}`,
          },
        };
      }

      const displayName = part.inlineData.displayName
        ? ` (${part.inlineData.displayName})`
        : '';
      return {
        type: 'text' as const,
        text: `Unsupported inline media type: ${mimeType}${displayName}.`,
      };
    }

    if (part.fileData?.mimeType && part.fileData?.fileUri) {
      const filename = part.fileData.displayName || 'file';
      const fileUri = part.fileData.fileUri;
      const mimeType = part.fileData.mimeType;
      const mediaType = this.getMediaType(mimeType);

      if (mediaType === 'image') {
        return {
          type: 'image_url' as const,
          image_url: { url: fileUri },
        };
      }

      if (mimeType === 'application/pdf') {
        return {
          type: 'file' as const,
          file: {
            filename,
            file_data: fileUri,
          },
        };
      }

      if (mediaType === 'video') {
        return {
          type: 'video_url' as const,
          video_url: {
            url: fileUri,
          },
        };
      }

      const displayName = part.fileData.displayName
        ? ` (${part.fileData.displayName})`
        : '';
      return {
        type: 'text' as const,
        text: `Unsupported file media type: ${mimeType}${displayName}.`,
      };
    }

    return null;
  }

  /**
   * Determine media type from MIME type
   */
  private getMediaType(mimeType: string): 'image' | 'audio' | 'video' | 'file' {
    if (mimeType.startsWith('image/')) return 'image';
    if (mimeType.startsWith('audio/')) return 'audio';
    if (mimeType.startsWith('video/')) return 'video';
    return 'file';
  }

  /**
   * Convert MIME type to OpenAI audio format
   */
  private getAudioFormat(mimeType: string): 'wav' | 'mp3' | null {
    if (mimeType.includes('wav')) return 'wav';
    if (mimeType.includes('mp3') || mimeType.includes('mpeg')) return 'mp3';
    return null;
  }

  /**
   * Extract text content from various Gemini content union types
   */
  private extractTextFromContentUnion(contentUnion: unknown): string {
    if (typeof contentUnion === 'string') {
      return contentUnion;
    }

    if (Array.isArray(contentUnion)) {
      return contentUnion
        .map((item) => this.extractTextFromContentUnion(item))
        .filter(Boolean)
        .join('\n');
    }

    if (typeof contentUnion === 'object' && contentUnion !== null) {
      if ('parts' in contentUnion) {
        const content = contentUnion as Content;
        return (
          content.parts
            ?.map((part: Part) => {
              if (typeof part === 'string') return part;
              if ('text' in part) return part.text || '';
              return '';
            })
            .filter(Boolean)
            .join('\n') || ''
        );
      }
    }

    return '';
  }

  /**
   * Convert OpenAI response to Gemini format
   */
  convertOpenAIResponseToGemini(
    openaiResponse: OpenAI.Chat.ChatCompletion,
  ): GenerateContentResponse {
    const choice = openaiResponse.choices[0];
    const response = new GenerateContentResponse();

    const parts: Part[] = [];

    // Handle reasoning content (thoughts)
    const reasoningText =
      (choice.message as ExtendedCompletionMessage).reasoning_content ??
      (choice.message as ExtendedCompletionMessage).reasoning;
    if (reasoningText) {
      parts.push({ text: reasoningText, thought: true });
    }

    // Handle text content
    if (choice.message.content) {
      parts.push({ text: choice.message.content });
    }

    // Handle tool calls
    if (choice.message.tool_calls) {
      for (const toolCall of choice.message.tool_calls) {
        if (toolCall.function) {
          let args: Record<string, unknown> = {};
          if (toolCall.function.arguments) {
            args = safeJsonParse(toolCall.function.arguments, {});
          }

          // Map tool call arguments to match expected parameter names
          args = this.mapToolCallArgs(toolCall.function.name, args);

          parts.push({
            functionCall: {
              id: toolCall.id,
              name: toolCall.function.name,
              args,
            },
          });
        }
      }
    }

    response.responseId = openaiResponse.id;
    response.createTime = openaiResponse.created
      ? openaiResponse.created.toString()
      : new Date().getTime().toString();

    response.candidates = [
      {
        content: {
          parts,
          role: 'model' as const,
        },
        finishReason: this.mapOpenAIFinishReasonToGemini(
          choice.finish_reason || 'stop',
        ),
        index: 0,
        safetyRatings: [],
      },
    ];

    response.modelVersion = this.model;
    response.promptFeedback = { safetyRatings: [] };

    // Add usage metadata if available
    if (openaiResponse.usage) {
      const usage = openaiResponse.usage;

      const promptTokens = usage.prompt_tokens || 0;
      const completionTokens = usage.completion_tokens || 0;
      const totalTokens = usage.total_tokens || 0;
      // Support both formats: prompt_tokens_details.cached_tokens (OpenAI standard)
      // and cached_tokens (some models return it at top level)
      const extendedUsage = usage as ExtendedCompletionUsage;
      const cachedTokens =
        usage.prompt_tokens_details?.cached_tokens ??
        extendedUsage.cached_tokens ??
        0;
      const thinkingTokens =
        usage.completion_tokens_details?.reasoning_tokens || 0;

      // If we only have total tokens but no breakdown, estimate the split
      // Typically input is ~70% and output is ~30% for most conversations
      let finalPromptTokens = promptTokens;
      let finalCompletionTokens = completionTokens;

      if (totalTokens > 0 && promptTokens === 0 && completionTokens === 0) {
        // Estimate: assume 70% input, 30% output
        finalPromptTokens = Math.round(totalTokens * 0.7);
        finalCompletionTokens = Math.round(totalTokens * 0.3);
      }

      response.usageMetadata = {
        promptTokenCount: finalPromptTokens,
        candidatesTokenCount: finalCompletionTokens,
        totalTokenCount: totalTokens,
        cachedContentTokenCount: cachedTokens,
        thoughtsTokenCount: thinkingTokens,
      };
    }

    return response;
  }

  /**
   * Convert OpenAI stream chunk to Gemini format
   */
  convertOpenAIChunkToGemini(
    chunk: OpenAI.Chat.ChatCompletionChunk,
  ): GenerateContentResponse {
    const choice = chunk.choices?.[0];
    const response = new GenerateContentResponse();

    if (choice) {
      const parts: Part[] = [];

      const reasoningText =
        (choice.delta as ExtendedCompletionChunkDelta)?.reasoning_content ??
        (choice.delta as ExtendedCompletionChunkDelta)?.reasoning;
      if (reasoningText) {
        parts.push({ text: reasoningText, thought: true });
      }

      // Handle text content
      if (choice.delta?.content) {
        if (typeof choice.delta.content === 'string') {
          parts.push({ text: choice.delta.content });
        }
      }

      // Handle tool calls using the streaming parser
      if (choice.delta?.tool_calls) {
        for (const toolCall of choice.delta.tool_calls) {
          const index = toolCall.index ?? 0;

          // Process the tool call chunk through the streaming parser
          if (toolCall.function?.arguments) {
            this.streamingToolCallParser.addChunk(
              index,
              toolCall.function.arguments,
              toolCall.id,
              toolCall.function.name,
            );
          } else {
            // Handle metadata-only chunks (id and/or name without arguments)
            this.streamingToolCallParser.addChunk(
              index,
              '', // Empty chunk for metadata-only updates
              toolCall.id,
              toolCall.function?.name,
            );
          }
        }
      }

      // Only emit function calls when streaming is complete (finish_reason is present)
      if (choice.finish_reason) {
        const completedToolCalls =
          this.streamingToolCallParser.getCompletedToolCalls();

        for (const toolCall of completedToolCalls) {
          if (toolCall.name) {
            // Map tool call arguments to match expected parameter names
            const mappedArgs = this.mapToolCallArgs(toolCall.name, toolCall.args);

            parts.push({
              functionCall: {
                id:
                  toolCall.id ||
                  `call_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`,
                name: toolCall.name,
                args: mappedArgs,
              },
            });
          }
        }

        // Clear the parser for the next stream
        this.streamingToolCallParser.reset();
      }

      // Only include finishReason key if finish_reason is present
      const candidate: Candidate = {
        content: {
          parts,
          role: 'model' as const,
        },
        index: 0,
        safetyRatings: [],
      };
      if (choice.finish_reason) {
        candidate.finishReason = this.mapOpenAIFinishReasonToGemini(
          choice.finish_reason,
        );
      }
      response.candidates = [candidate];
    } else {
      response.candidates = [];
    }

    response.responseId = chunk.id;
    response.createTime = chunk.created
      ? chunk.created.toString()
      : new Date().getTime().toString();

    response.modelVersion = this.model;
    response.promptFeedback = { safetyRatings: [] };

    // Add usage metadata if available in the chunk
    if (chunk.usage) {
      const usage = chunk.usage;

      const promptTokens = usage.prompt_tokens || 0;
      const completionTokens = usage.completion_tokens || 0;
      const totalTokens = usage.total_tokens || 0;
      const thinkingTokens =
        usage.completion_tokens_details?.reasoning_tokens || 0;
      // Support both formats: prompt_tokens_details.cached_tokens (OpenAI standard)
      // and cached_tokens (some models return it at top level)
      const extendedUsage = usage as ExtendedCompletionUsage;
      const cachedTokens =
        usage.prompt_tokens_details?.cached_tokens ??
        extendedUsage.cached_tokens ??
        0;

      // If we only have total tokens but no breakdown, estimate the split
      // Typically input is ~70% and output is ~30% for most conversations
      let finalPromptTokens = promptTokens;
      let finalCompletionTokens = completionTokens;

      if (totalTokens > 0 && promptTokens === 0 && completionTokens === 0) {
        // Estimate: assume 70% input, 30% output
        finalPromptTokens = Math.round(totalTokens * 0.7);
        finalCompletionTokens = Math.round(totalTokens * 0.3);
      }

      response.usageMetadata = {
        promptTokenCount: finalPromptTokens,
        candidatesTokenCount: finalCompletionTokens,
        thoughtsTokenCount: thinkingTokens,
        totalTokenCount: totalTokens,
        cachedContentTokenCount: cachedTokens,
      };
    }

    return response;
  }

  /**
   * Map OpenAI finish reasons to Gemini finish reasons
   */
  private mapOpenAIFinishReasonToGemini(
    openaiReason: string | null,
  ): FinishReason {
    if (!openaiReason) return FinishReason.FINISH_REASON_UNSPECIFIED;
    const mapping: Record<string, FinishReason> = {
      stop: FinishReason.STOP,
      length: FinishReason.MAX_TOKENS,
      content_filter: FinishReason.SAFETY,
      function_call: FinishReason.STOP,
      tool_calls: FinishReason.STOP,
    };
    return mapping[openaiReason] || FinishReason.FINISH_REASON_UNSPECIFIED;
  }

  /**
   * Clean up orphaned tool calls from message history to prevent OpenAI API errors
   */
  private cleanOrphanedToolCalls(
    messages: OpenAI.Chat.ChatCompletionMessageParam[],
  ): OpenAI.Chat.ChatCompletionMessageParam[] {
    const cleaned: OpenAI.Chat.ChatCompletionMessageParam[] = [];
    const toolCallIds = new Set<string>();
    const toolResponseIds = new Set<string>();

    // First pass: collect all tool call IDs and tool response IDs
    for (const message of messages) {
      if (
        message.role === 'assistant' &&
        'tool_calls' in message &&
        message.tool_calls
      ) {
        for (const toolCall of message.tool_calls) {
          if (toolCall.id) {
            toolCallIds.add(toolCall.id);
          }
        }
      } else if (
        message.role === 'tool' &&
        'tool_call_id' in message &&
        message.tool_call_id
      ) {
        toolResponseIds.add(message.tool_call_id);
      }
    }

    // Second pass: filter out orphaned messages
    for (const message of messages) {
      if (
        message.role === 'assistant' &&
        'tool_calls' in message &&
        message.tool_calls
      ) {
        // Filter out tool calls that don't have corresponding responses
        const validToolCalls = message.tool_calls.filter(
          (toolCall) => toolCall.id && toolResponseIds.has(toolCall.id),
        );

        if (validToolCalls.length > 0) {
          // Keep the message but only with valid tool calls
          const cleanedMessage = { ...message };
          (
            cleanedMessage as OpenAI.Chat.ChatCompletionMessageParam & {
              tool_calls?: OpenAI.Chat.ChatCompletionMessageToolCall[];
            }
          ).tool_calls = validToolCalls;
          cleaned.push(cleanedMessage);
        } else if (
          typeof message.content === 'string' &&
          message.content.trim()
        ) {
          // Keep the message if it has text content, but remove tool calls
          const cleanedMessage = { ...message };
          delete (
            cleanedMessage as OpenAI.Chat.ChatCompletionMessageParam & {
              tool_calls?: OpenAI.Chat.ChatCompletionMessageToolCall[];
            }
          ).tool_calls;
          cleaned.push(cleanedMessage);
        }
        // If no valid tool calls and no content, skip the message entirely
      } else if (
        message.role === 'tool' &&
        'tool_call_id' in message &&
        message.tool_call_id
      ) {
        // Only keep tool responses that have corresponding tool calls
        if (toolCallIds.has(message.tool_call_id)) {
          cleaned.push(message);
        }
      } else {
        // Keep all other messages as-is
        cleaned.push(message);
      }
    }

    // Final validation: ensure every assistant message with tool_calls has corresponding tool responses
    const finalCleaned: OpenAI.Chat.ChatCompletionMessageParam[] = [];
    const finalToolCallIds = new Set<string>();

    // Collect all remaining tool call IDs
    for (const message of cleaned) {
      if (
        message.role === 'assistant' &&
        'tool_calls' in message &&
        message.tool_calls
      ) {
        for (const toolCall of message.tool_calls) {
          if (toolCall.id) {
            finalToolCallIds.add(toolCall.id);
          }
        }
      }
    }

    // Verify all tool calls have responses
    const finalToolResponseIds = new Set<string>();
    for (const message of cleaned) {
      if (
        message.role === 'tool' &&
        'tool_call_id' in message &&
        message.tool_call_id
      ) {
        finalToolResponseIds.add(message.tool_call_id);
      }
    }

    // Remove any remaining orphaned tool calls
    for (const message of cleaned) {
      if (
        message.role === 'assistant' &&
        'tool_calls' in message &&
        message.tool_calls
      ) {
        const finalValidToolCalls = message.tool_calls.filter(
          (toolCall) => toolCall.id && finalToolResponseIds.has(toolCall.id),
        );

        if (finalValidToolCalls.length > 0) {
          const cleanedMessage = { ...message };
          (
            cleanedMessage as OpenAI.Chat.ChatCompletionMessageParam & {
              tool_calls?: OpenAI.Chat.ChatCompletionMessageToolCall[];
            }
          ).tool_calls = finalValidToolCalls;
          finalCleaned.push(cleanedMessage);
        } else if (
          typeof message.content === 'string' &&
          message.content.trim()
        ) {
          const cleanedMessage = { ...message };
          delete (
            cleanedMessage as OpenAI.Chat.ChatCompletionMessageParam & {
              tool_calls?: OpenAI.Chat.ChatCompletionMessageToolCall[];
            }
          ).tool_calls;
          finalCleaned.push(cleanedMessage);
        }
      } else {
        finalCleaned.push(message);
      }
    }

    return finalCleaned;
  }

  /**
   * Merge consecutive assistant messages to combine split text and tool calls
   */
  private mergeConsecutiveAssistantMessages(
    messages: OpenAI.Chat.ChatCompletionMessageParam[],
  ): OpenAI.Chat.ChatCompletionMessageParam[] {
    const merged: OpenAI.Chat.ChatCompletionMessageParam[] = [];

    for (const message of messages) {
      if (message.role === 'assistant' && merged.length > 0) {
        const lastMessage = merged[merged.length - 1];

        // If the last message is also an assistant message, merge them
        if (lastMessage.role === 'assistant') {
          const lastToolCalls =
            'tool_calls' in lastMessage ? lastMessage.tool_calls || [] : [];
          const currentToolCalls =
            'tool_calls' in message ? message.tool_calls || [] : [];
          // Combine content
          const lastContent = lastMessage.content;
          const currentContent = message.content;

          // Determine if we should use array format (if either content is an array)
          const useArrayFormat =
            Array.isArray(lastContent) || Array.isArray(currentContent);

          let combinedContent:
            | string
            | OpenAI.Chat.ChatCompletionContentPart[]
            | null;

          if (useArrayFormat) {
            // Convert both to array format and merge
            const lastParts = Array.isArray(lastContent)
              ? lastContent
              : typeof lastContent === 'string' && lastContent
                ? [{ type: 'text' as const, text: lastContent }]
                : [];

            const currentParts = Array.isArray(currentContent)
              ? currentContent
              : typeof currentContent === 'string' && currentContent
                ? [{ type: 'text' as const, text: currentContent }]
                : [];

            combinedContent = [
              ...lastParts,
              ...currentParts,
            ] as OpenAI.Chat.ChatCompletionContentPart[];
          } else {
            // Both are strings or null, merge as strings
            const lastText = typeof lastContent === 'string' ? lastContent : '';
            const currentText =
              typeof currentContent === 'string' ? currentContent : '';
            const mergedText = [lastText, currentText].filter(Boolean).join('');
            combinedContent = mergedText || null;
          }

          // Combine tool calls
          const combinedToolCalls = [...lastToolCalls, ...currentToolCalls];

          // Update the last message with combined data
          (
            lastMessage as OpenAI.Chat.ChatCompletionMessageParam & {
              content: string | OpenAI.Chat.ChatCompletionContentPart[] | null;
              tool_calls?: OpenAI.Chat.ChatCompletionMessageToolCall[];
            }
          ).content = combinedContent || null;
          if (combinedToolCalls.length > 0) {
            (
              lastMessage as OpenAI.Chat.ChatCompletionMessageParam & {
                content:
                  | string
                  | OpenAI.Chat.ChatCompletionContentPart[]
                  | null;
                tool_calls?: OpenAI.Chat.ChatCompletionMessageToolCall[];
              }
            ).tool_calls = combinedToolCalls;
          }

          continue; // Skip adding the current message since it's been merged
        }
      }

      // Add the message as-is if no merging is needed
      merged.push(message);
    }

    return merged;
  }
}
